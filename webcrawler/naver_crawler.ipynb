{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "class NaverCafeCrawler:\n",
    "    def __init__(self, driver_path, url, id, pw, baseurl, clubid, userDisplay, boardType):\n",
    "        self.total_list = ['제목', '내용', '링크']\n",
    "        self.driver_path = driver_path\n",
    "        self.url = url\n",
    "        self.id = id\n",
    "        self.pw = pw\n",
    "        self.baseurl = baseurl\n",
    "        self.baseraw = \"https://cafe.naver.com\"\n",
    "        self.clubid = clubid\n",
    "        self.userDisplay = userDisplay\n",
    "        self.boardType = boardType\n",
    "\n",
    "    def initialize_file(self, file_path='crawl.csv'):\n",
    "        with open(file_path, 'w', encoding='utf-8', newline='') as f:\n",
    "            wr = csv.writer(f)\n",
    "            wr.writerow([self.total_list[0], self.total_list[1], self.total_list[2]])\n",
    "\n",
    "    def login(self, browser):\n",
    "        browser.get(self.url)\n",
    "        browser.implicitly_wait(2)\n",
    "        browser.execute_script(f\"document.getElementsByName('id')[0].value='{self.id}'\")\n",
    "        browser.execute_script(f\"document.getElementsByName('pw')[0].value='{self.pw}'\")\n",
    "        browser.find_element(By.XPATH, '//*[@id=\"log.login\"]').click()\n",
    "        time.sleep(1)\n",
    "\n",
    "    def crawl_page(self, browser, page_num):\n",
    "        browser.get(f\"{self.baseurl}ArticleList.nhn?search.clubid={self.clubid}&userDisplay={self.userDisplay}\"\n",
    "                    f\"&search.boardType={self.boardType}&search.page={page_num}\")\n",
    "        browser.switch_to.frame('cafe_main')\n",
    "        soup = bs(browser.page_source, 'html.parser')\n",
    "        soup = soup.find_all(class_='article-board m-tcol-c')[1]\n",
    "        datas = soup.find_all(class_='td_article')\n",
    "        new_df = pd.DataFrame(columns=['제목', '내용', '링크'])\n",
    "        for data in datas:\n",
    "            article_title = data.find(class_='article')\n",
    "            link = article_title.get('href')\n",
    "            article_title = article_title.get_text().strip()\n",
    "            content = self.get_content(browser, self.baseraw + link)\n",
    "            new_df = pd.concat([new_df, pd.DataFrame({'제목': [article_title], '내용': [content], '링크': [self.baseraw + link]})],\n",
    "                              ignore_index=True)\n",
    "        return new_df\n",
    "\n",
    "    def get_content(self, browser, link):\n",
    "        browser.get(link)\n",
    "        time.sleep(1)\n",
    "        browser.switch_to.frame('cafe_main')\n",
    "        soup = bs(browser.page_source, 'html.parser')\n",
    "\n",
    "        content_div = soup.find(\"div\", {\"class\": \"article_viewer\"})\n",
    "        if content_div:\n",
    "            content = content_div.get_text().strip()\n",
    "        else:\n",
    "            content = \"\"\n",
    "\n",
    "        comments_div = soup.find(\"div\", {\"class\": \"comment_text_box\"})\n",
    "        if comments_div:\n",
    "            comments = []\n",
    "            for comment in comments_div.find_all(\"span\", {\"class\": \"text_comment\"}):\n",
    "                comment_text = comment.get_text().strip()\n",
    "                nested_replies = self.extract_replies(comment)\n",
    "                comment_text += \"\\nReplies:\\n\" + nested_replies\n",
    "                comments.append(comment_text)\n",
    "\n",
    "            comments = \"\\n\\n\".join(comments)\n",
    "            print(comments)\n",
    "            content += \"\\n\\nComments:\\n\" + comments\n",
    "\n",
    "        return content\n",
    "\n",
    "    def extract_replies(self, comment):\n",
    "        replies = []\n",
    "        nested_replies_div = comment.find_next(\"div\", {\"class\": \"comment_text_box\"})\n",
    "        while nested_replies_div:\n",
    "            reply_text = nested_replies_div.find(\"span\", {\"class\": \"text_comment\"}).get_text().strip()\n",
    "            nested_replies = self.extract_replies(nested_replies_div)\n",
    "            reply_text += \"\\nReplies:\\n\" + nested_replies\n",
    "            replies.append(reply_text)\n",
    "            nested_replies_div = nested_replies_div.find_next(\"div\", {\"class\": \"comment_text_box\"})\n",
    "\n",
    "        return \"\\n\\n\".join(replies)\n",
    "\n",
    "    def run(self, max_pages=2, file_path='crawl.csv'):\n",
    "        self.initialize_file(file_path)\n",
    "        i = 0\n",
    "        while i < max_pages:\n",
    "            print(f\"Processing page {i}....\")\n",
    "            i += 1\n",
    "            pageNum = i\n",
    "            original_df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument('--headless')\n",
    "            chrome_options.add_argument(f'--webdriver-path={self.driver_path}')\n",
    "            browser = webdriver.Chrome(options=chrome_options)\n",
    "            self.login(browser)\n",
    "            new_df = self.crawl_page(browser, pageNum)\n",
    "            concat_df = pd.concat([original_df, new_df])\n",
    "            concat_df = concat_df.drop_duplicates(keep=False)\n",
    "            concat_df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "            browser.close()\n",
    "        print(\"done completely....\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    driver_path = \"Chrome-Driver-Path 지정\"   #### 사용자 정의\n",
    "    url = 'https://nid.naver.com/nidlogin.login'\n",
    "    id = \"네이버ID\" #### 사용자 정의\n",
    "    pw = \"네이버패스워드\" #### 사용자 정의\n",
    "    baseurl = \"https://cafe.naver.com/\"\n",
    "    clubid = 카페ID지정 #### 사용자 정의\n",
    "    userDisplay = 50\n",
    "    boardType = 'L'\n",
    "\n",
    "    crawler = NaverCafeCrawler(driver_path, url, id, pw, baseurl, clubid, userDisplay, boardType)\n",
    "    crawler.run(max_pages=1) #### 사용자 정의 - 페이지 크기\n",
    "    df = pd.read_csv(\"crawl.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rpython1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
